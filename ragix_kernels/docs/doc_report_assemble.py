"""
Kernel: Document Report Assembly
Stage: 3 (Report)

Assembles all document summarization outputs into a complete report:
- YAML frontmatter with metadata
- Table of contents
- Corpus overview
- Domain sections with summaries
- Cluster/group details
- Document inventory
- Coverage analysis
- Appendices

Output formats:
- Markdown (primary)
- HTML (optional, via pandoc)

Author: Olivier Vitrac, PhD, HDR | olivier.vitrac@adservio.fr | Adservio | 2025-01-18
"""

from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from collections import defaultdict
import logging
import json
import subprocess

from ragix_kernels.base import Kernel, KernelInput

logger = logging.getLogger(__name__)


class DocReportAssembleKernel(Kernel):
    """
    Assemble complete document summarization report.

    This kernel combines all document analysis outputs into a single,
    well-structured report suitable for stakeholder review.

    Configuration options:
        language: Report language ("en" or "fr", default: "fr")
        title: Report title (default: auto-generated)
        author: Report author
        client: Client name
        date: Report date (default: today)
        include_toc: Include table of contents (default: true)
        include_appendix: Include detailed appendices (default: true)
        max_documents_detail: Max documents to show in detail (default: 30)
        output_formats: List of formats ["md", "html"] (default: ["md"])

    Dependencies:
        doc_metadata: File inventory
        doc_concepts: Concept mappings
        doc_structure: Document outlines
        doc_cluster: Document clusters
        doc_extract: Key sentences
        doc_coverage: Coverage analysis
        doc_pyramid: Hierarchical summary

    Output:
        report_md: Path to Markdown report
        report_html: Path to HTML report (if enabled)
        report_content: Full Markdown content
        metadata: Report metadata
    """

    name = "doc_report_assemble"
    version = "1.0.0"
    category = "docs"
    stage = 3
    description = "Assemble complete document summarization report"

    requires = [
        "doc_metadata", "doc_concepts", "doc_cluster",
        "doc_coverage", "doc_extract", "doc_pyramid"
    ]
    provides = ["doc_report_md", "doc_report_html", "report_metadata"]

    # i18n strings
    I18N = {
        "en": {
            "report_title": "Document Analysis Report",
            "toc_title": "Table of Contents",
            "executive_summary": "Executive Summary",
            "corpus_overview": "Corpus Overview",
            "key_findings": "Key Findings",
            "domains": "Thematic Domains",
            "clusters": "Document Clusters",
            "coverage": "Coverage Analysis",
            "gaps": "Coverage Gaps",
            "overlaps": "High-Coverage Concepts",
            "documents": "Document Inventory",
            "appendix": "Appendices",
            "concepts_appendix": "Concept Index",
            "files": "files",
            "chunks": "chunks",
            "concepts": "concepts",
            "sentences": "sentences",
            "sections": "sections",
            "generated_by": "Generated by KOAS Document Summarization",
            "generated_at": "Generated at",
            "total_size": "Total Size",
            "avg_per_file": "Average per file",
            "top_concepts": "Top Concepts",
            "related_concepts": "Related Concepts",
            "representative_content": "Representative Content",
            "document_list": "Documents in this cluster",
        },
        "fr": {
            "report_title": "Rapport d'Analyse Documentaire",
            "toc_title": "Table des Matières",
            "executive_summary": "Synthèse",
            "corpus_overview": "Vue d'Ensemble du Corpus",
            "key_findings": "Constats Clés",
            "domains": "Domaines Thématiques",
            "clusters": "Groupes de Documents",
            "coverage": "Analyse de Couverture",
            "gaps": "Lacunes de Couverture",
            "overlaps": "Concepts Transversaux",
            "documents": "Inventaire des Documents",
            "appendix": "Annexes",
            "concepts_appendix": "Index des Concepts",
            "files": "fichiers",
            "chunks": "segments",
            "concepts": "concepts",
            "sentences": "phrases",
            "sections": "sections",
            "generated_by": "Généré par KOAS Document Summarization",
            "generated_at": "Généré le",
            "total_size": "Taille Totale",
            "avg_per_file": "Moyenne par fichier",
            "top_concepts": "Concepts Principaux",
            "related_concepts": "Concepts Associés",
            "representative_content": "Contenu Représentatif",
            "document_list": "Documents dans ce groupe",
        },
    }

    def compute(self, input: KernelInput) -> Dict[str, Any]:
        """Assemble complete document report."""
        start_time = datetime.now()

        # Configuration
        language = input.config.get("language", "fr")
        title = input.config.get("title")
        author = input.config.get("author", "KOAS")
        client = input.config.get("client", "")
        report_date = input.config.get("date", datetime.now().strftime("%Y-%m-%d"))
        include_toc = input.config.get("include_toc", True)
        include_appendix = input.config.get("include_appendix", True)
        max_docs_detail = input.config.get("max_documents_detail", 30)
        output_formats = input.config.get("output_formats", ["md"])
        project_config = input.config.get("project", {})

        i18n = self.I18N.get(language, self.I18N["fr"])

        logger.info(f"[doc_report_assemble] Assembling report in {language}")

        # Load all dependencies
        deps = self._load_all_dependencies(input)

        # Extract key data
        pyramid = deps["doc_pyramid"].get("pyramid", {})
        corpus = pyramid.get("level_4_corpus", {})
        domains = pyramid.get("level_3_domains", [])
        groups = pyramid.get("level_2_groups", [])
        documents = pyramid.get("level_1_documents", [])

        metadata_stats = deps["doc_metadata"].get("statistics", {})
        coverage_data = deps["doc_coverage"]
        gaps = coverage_data.get("gaps", [])
        overlaps = coverage_data.get("overlaps", [])

        # Auto-generate title if not provided
        if not title:
            project_name = project_config.get("name", corpus.get("title", ""))
            title = f"{i18n['report_title']} - {project_name}" if project_name else i18n["report_title"]

        # Build report
        lines = []

        # 1. YAML Frontmatter
        lines.extend(self._build_frontmatter(title, author, client, report_date, language))

        # 2. Title
        lines.append(f"# {title}\n")

        # 3. Table of Contents
        if include_toc:
            lines.extend(self._build_toc(i18n, domains, include_appendix))

        # 4. Executive Summary / Corpus Overview
        lines.extend(self._build_executive_summary(corpus, metadata_stats, i18n))

        # 5. Key Findings
        lines.extend(self._build_key_findings(corpus, gaps, overlaps, i18n))

        # 6. Domain Sections
        lines.extend(self._build_domains_section(domains, i18n))

        # 7. Clusters Section
        lines.extend(self._build_clusters_section(groups, i18n, max_docs_detail))

        # 8. Coverage Analysis
        lines.extend(self._build_coverage_section(coverage_data, gaps, overlaps, i18n))

        # 9. Document Inventory (summary)
        lines.extend(self._build_documents_section(documents, metadata_stats, i18n, max_docs_detail))

        # 10. Appendices
        if include_appendix:
            lines.extend(self._build_appendices(deps, i18n))

        # 11. Footer
        lines.extend(self._build_footer(i18n))

        # Join content
        report_content = "\n".join(lines)

        # Save Markdown
        output_dir = input.workspace / f"stage{self.stage}"
        md_path = output_dir / "doc_report.md"
        md_path.write_text(report_content, encoding="utf-8")
        logger.info(f"[doc_report_assemble] Saved Markdown: {md_path}")

        # Generate HTML if requested
        html_path = None
        if "html" in output_formats:
            html_path = self._generate_html(md_path, output_dir, title)

        # Metadata
        processing_time = int((datetime.now() - start_time).total_seconds() * 1000)
        metadata = {
            "title": title,
            "author": author,
            "client": client,
            "date": report_date,
            "language": language,
            "sections": {
                "domains": len(domains),
                "clusters": len(groups),
                "documents": len(documents),
            },
            "output_files": {
                "markdown": str(md_path),
                "html": str(html_path) if html_path else None,
            },
            "processing_time_ms": processing_time,
        }

        return {
            "report_md": str(md_path),
            "report_html": str(html_path) if html_path else None,
            "report_content": report_content,
            "metadata": metadata,
        }

    def _load_all_dependencies(self, input: KernelInput) -> Dict[str, Any]:
        """Load all dependency data."""
        deps = {}
        for name in self.requires:
            path = input.dependencies.get(name)
            if path and path.exists():
                with open(path) as f:
                    deps[name] = json.load(f).get("data", {})
            else:
                deps[name] = {}
        return deps

    def _build_frontmatter(
        self, title: str, author: str, client: str, date: str, language: str
    ) -> List[str]:
        """Build YAML frontmatter."""
        lines = [
            "---",
            f"title: \"{title}\"",
            f"author: \"{author}\"",
        ]
        if client:
            lines.append(f"client: \"{client}\"")
        lines.extend([
            f"date: \"{date}\"",
            f"lang: \"{language}\"",
            "documentclass: report",
            "toc: true",
            "---",
            "",
        ])
        return lines

    def _build_toc(self, i18n: Dict, domains: List, include_appendix: bool) -> List[str]:
        """Build table of contents."""
        lines = [
            f"## {i18n['toc_title']}",
            "",
            f"1. [{i18n['executive_summary']}](#synthèse)",
            f"2. [{i18n['key_findings']}](#constats-clés)",
            f"3. [{i18n['domains']}](#domaines-thématiques)",
        ]

        # Add domain links
        for i, domain in enumerate(domains[:10], 1):
            label = domain.get("label", f"Domain {i}")
            anchor = label.lower().replace(" ", "-").replace(".", "")[:30]
            lines.append(f"   - [{label}](#{anchor})")

        lines.extend([
            f"4. [{i18n['clusters']}](#groupes-de-documents)",
            f"5. [{i18n['coverage']}](#analyse-de-couverture)",
            f"6. [{i18n['documents']}](#inventaire-des-documents)",
        ])

        if include_appendix:
            lines.append(f"7. [{i18n['appendix']}](#annexes)")

        lines.append("")
        return lines

    def _build_executive_summary(
        self, corpus: Dict, stats: Dict, i18n: Dict
    ) -> List[str]:
        """Build executive summary section."""
        lines = [
            f"## {i18n['executive_summary']}",
            "",
            f"### {i18n['corpus_overview']}",
            "",
        ]

        # Key metrics table
        file_count = corpus.get("file_count", stats.get("total_files", 0))
        chunk_count = corpus.get("chunk_count", stats.get("total_chunks", 0))
        size_mb = corpus.get("total_size_mb", stats.get("total_size_mb", 0))
        concept_count = corpus.get("concept_count", 0)
        domain_count = corpus.get("domain_count", 0)

        lines.extend([
            "| Métrique | Valeur |",
            "|----------|--------|",
            f"| **Documents** | {file_count} {i18n['files']} |",
            f"| **Segments** | {chunk_count} {i18n['chunks']} |",
            f"| **{i18n['total_size']}** | {size_mb} MB |",
            f"| **Concepts** | {concept_count} |",
            f"| **Domaines** | {domain_count} |",
            "",
        ])

        # File types breakdown
        file_types = corpus.get("file_types", stats.get("kind_counts", {}))
        if file_types:
            lines.append("**Types de documents:**")
            type_str = ", ".join(
                f"{k.replace('doc_', '').upper()}: {v}"
                for k, v in sorted(file_types.items(), key=lambda x: -x[1])[:5]
            )
            lines.extend([type_str, ""])

        return lines

    def _build_key_findings(
        self, corpus: Dict, gaps: List, overlaps: List, i18n: Dict
    ) -> List[str]:
        """Build key findings section."""
        lines = [
            f"## {i18n['key_findings']}",
            "",
            f"### {i18n['top_concepts']}",
            "",
        ]

        # Top concepts
        key_concepts = corpus.get("key_concepts", [])
        for concept in key_concepts[:10]:
            lines.append(f"- {concept}")
        lines.append("")

        # Coverage gaps
        if gaps:
            lines.extend([
                f"### {i18n['gaps']}",
                "",
                f"*{len(gaps)} concepts avec couverture insuffisante:*",
                "",
            ])
            for gap in gaps[:10]:
                label = gap.get("label", "")
                count = gap.get("file_count", 0)
                lines.append(f"- **{label}** ({count} {i18n['files']})")
            lines.append("")

        # Overlapping concepts
        if overlaps:
            lines.extend([
                f"### {i18n['overlaps']}",
                "",
                f"*Concepts présents dans >50% des documents:*",
                "",
            ])
            for overlap in overlaps[:5]:
                label = overlap.get("label", "")
                pct = overlap.get("file_percentage", 0)
                lines.append(f"- **{label}** ({pct}%)")
            lines.append("")

        return lines

    def _build_domains_section(self, domains: List, i18n: Dict) -> List[str]:
        """Build domains section."""
        lines = [
            f"## {i18n['domains']}",
            "",
        ]

        for domain in domains:
            label = domain.get("label", "Unknown")
            file_count = domain.get("file_count", 0)
            concept_count = domain.get("concept_count", 0)

            lines.extend([
                f"### {label}",
                "",
                f"**{file_count}** {i18n['files']} | **{concept_count}** {i18n['concepts']}",
                "",
            ])

            # Related concepts
            related = domain.get("related_concepts", [])
            if related:
                lines.append(f"**{i18n['related_concepts']}:** {', '.join(related[:8])}")
                lines.append("")

            # Representative sentences
            sentences = domain.get("representative_sentences", [])
            if sentences:
                lines.append(f"**{i18n['representative_content']}:**")
                lines.append("")
                for sent in sentences[:3]:
                    lines.append(f"> {sent}")
                    lines.append("")

            lines.append("---")
            lines.append("")

        return lines

    def _build_clusters_section(
        self, groups: List, i18n: Dict, max_docs: int
    ) -> List[str]:
        """Build clusters section."""
        lines = [
            f"## {i18n['clusters']}",
            "",
        ]

        for group in groups:
            label = group.get("label", "Unknown")
            file_count = group.get("file_count", 0)
            centroid = group.get("centroid_concepts", [])

            lines.extend([
                f"### {label}",
                "",
                f"**{file_count}** {i18n['files']}",
                "",
            ])

            if centroid:
                lines.append(f"**Focus:** {', '.join(centroid[:5])}")
                lines.append("")

            # Document list (abbreviated)
            file_ids = group.get("file_ids", [])
            file_paths = group.get("file_paths", [])
            if file_paths:
                lines.append(f"**{i18n['document_list']}:**")
                lines.append("")
                for path in file_paths[:5]:
                    filename = Path(path).name
                    lines.append(f"- {filename}")
                if len(file_paths) > 5:
                    lines.append(f"- *... et {len(file_paths) - 5} autres*")
                lines.append("")

            lines.append("")

        return lines

    def _build_coverage_section(
        self, coverage_data: Dict, gaps: List, overlaps: List, i18n: Dict
    ) -> List[str]:
        """Build coverage analysis section."""
        stats = coverage_data.get("statistics", {})

        lines = [
            f"## {i18n['coverage']}",
            "",
            "| Métrique | Valeur |",
            "|----------|--------|",
            f"| Concepts totaux | {stats.get('total_concepts', 0)} |",
            f"| Paires couverture | {stats.get('total_coverage_pairs', 0)} |",
            f"| Moyenne concepts/fichier | {stats.get('avg_concepts_per_file', 0)} |",
            f"| Lacunes | {stats.get('gaps_count', 0)} |",
            f"| Chevauchements | {stats.get('overlaps_count', 0)} |",
            "",
        ]

        return lines

    def _build_documents_section(
        self, documents: List, stats: Dict, i18n: Dict, max_docs: int
    ) -> List[str]:
        """Build document inventory section."""
        lines = [
            f"## {i18n['documents']}",
            "",
            f"*{len(documents)} documents indexés*",
            "",
            "| Document | Type | Concepts | Segments |",
            "|----------|------|----------|----------|",
        ]

        for doc in documents[:max_docs]:
            path = Path(doc.get("path", "")).name
            kind = doc.get("kind", "").replace("doc_", "").upper()
            concept_count = doc.get("concept_count", 0)
            chunk_count = doc.get("chunk_count", 0)
            lines.append(f"| {path[:40]} | {kind} | {concept_count} | {chunk_count} |")

        if len(documents) > max_docs:
            lines.append(f"| *... et {len(documents) - max_docs} autres* | | | |")

        lines.append("")
        return lines

    def _build_appendices(self, deps: Dict, i18n: Dict) -> List[str]:
        """Build appendices section."""
        lines = [
            f"## {i18n['appendix']}",
            "",
            f"### {i18n['concepts_appendix']}",
            "",
        ]

        # Concept list
        concepts = deps.get("doc_concepts", {}).get("concepts", [])
        if concepts:
            lines.append("| Concept | Fichiers | Score |")
            lines.append("|---------|----------|-------|")
            for c in concepts[:50]:
                label = c.get("label", "")
                file_count = c.get("file_count", 0)
                score = c.get("total_score", 0)
                lines.append(f"| {label} | {file_count} | {score:.1f} |")
            lines.append("")

        return lines

    def _build_footer(self, i18n: Dict) -> List[str]:
        """Build report footer."""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return [
            "---",
            "",
            f"*{i18n['generated_by']}*",
            "",
            f"*{i18n['generated_at']}: {timestamp}*",
        ]

    def _generate_html(
        self, md_path: Path, output_dir: Path, title: str
    ) -> Optional[Path]:
        """Generate HTML from Markdown using pandoc."""
        html_path = output_dir / "doc_report.html"

        try:
            result = subprocess.run(
                [
                    "pandoc",
                    str(md_path),
                    "-o", str(html_path),
                    "--standalone",
                    "--toc",
                    f"--metadata=title:{title}",
                    "--css=https://cdn.simplecss.org/simple.min.css",
                ],
                capture_output=True,
                text=True,
                timeout=30,
            )
            if result.returncode == 0:
                logger.info(f"[doc_report_assemble] Saved HTML: {html_path}")
                return html_path
            else:
                logger.warning(f"[doc_report_assemble] pandoc failed: {result.stderr}")
                return None
        except FileNotFoundError:
            logger.warning("[doc_report_assemble] pandoc not found, skipping HTML")
            return None
        except Exception as e:
            logger.warning(f"[doc_report_assemble] HTML generation failed: {e}")
            return None

    def summarize(self, data: Dict[str, Any]) -> str:
        """Generate human-readable summary."""
        meta = data.get("metadata", {})
        sections = meta.get("sections", {})
        time_ms = meta.get("processing_time_ms", 0)

        return (
            f"Report assembled: {sections.get('domains', 0)} domains, "
            f"{sections.get('clusters', 0)} clusters, "
            f"{sections.get('documents', 0)} documents. "
            f"Generated in {time_ms}ms."
        )
